{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8BZI9zlTy1G"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_train, x_test, y_train, y_test = np.load('regression_data.npy', allow_pickle=True)\n",
        "\n",
        "# Function to compute lost\n",
        "def compute_loss(x, y, a, b):\n",
        "    intercept = y - ( a * x + b)\n",
        "    loss = intercept ** 2\n",
        "    return loss\n",
        "\n",
        "#Training\n",
        "MSE_train = []\n",
        "a = 20\n",
        "b = 0\n",
        "learning_rate = 0.005\n",
        "i = 0\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "    \n",
        "    # Feed foward the training data into the model, get the output prediction\n",
        "    # Compute sum of square error\n",
        "    # MSE_train[i] = MSE after i-th updating\n",
        "    ll = 0     \n",
        "    \n",
        "    for j in range(len(x_train)):\n",
        "        ll += compute_loss(x_train[j], y_train[j], a, b)    \n",
        "    MSE_train.append(ll/750)\n",
        "    \n",
        "    # print(a,b,MSE_train[i]) # Print to check corresponding MSE with such weights and intercepts \n",
        "    \n",
        "    # Calculating the gradients\n",
        "    gradient_a = (-2) * x_train[i] * (y_train[i] - a * x_train[i] + b)\n",
        "    gradient_b = 2 * (y_train[i] - a * x_train[i] + b )\n",
        "    \n",
        "    # Updating the weights and intercepts by the gradients * learning rate\n",
        "    a -= learning_rate * gradient_a\n",
        "    b -= learning_rate * gradient_b\n",
        "    \n",
        "# Compute MSE of test data\n",
        "total_loss = 0    \n",
        "for i in range(len(x_test)):\n",
        "    total_loss += compute_loss(x_test[i], y_test[i], a, b)    \n",
        "MSE_test = total_loss/len(x_test)\n",
        "\n",
        "# Learning curve\n",
        "plt.plot(range(750), MSE_train, '-') \n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "print('Mean Square Error of my prediction and ground truth:', MSE_test)\n",
        "print('Weight:', a)\n",
        "print('Intercept:', b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_train, x_test, y_train, y_test = np.load('classification_data.npy', allow_pickle=True)\n",
        "\n",
        "cost = []\n",
        "w0 = 0\n",
        "w1 = 0\n",
        "learning_rate = 0.25\n",
        "\n",
        "# Function to compute sigmoid\n",
        "def sigmoid(x):\n",
        "    z = np.exp(-x)\n",
        "    sig = 1 / (1 + z)\n",
        "    return sig\n",
        "    \n",
        "i = 0 \n",
        "for i in range(len(x_train)):\n",
        "   \n",
        "    # Compute loss in the training process to check the learning curve\n",
        "    cost_sum = 0\n",
        "    for j in range(len(x_train)):\n",
        "        cost_sum += ((-y_train[j]) * np.log(sigmoid(w0 + w1 * x_train[j])) - (1 - y_train[j]) * np.log(1-sigmoid(w0 + w1 * x_train[j])))\n",
        "    cost.append(cost_sum)\n",
        "    \n",
        "    # Calculating the gradients\n",
        "    gradient_w0 =  sigmoid(w0 + w1 * x_train[i]) - y_train[i] \n",
        "    gradient_w1 = (sigmoid(w0 + w1 * x_train[i]) - y_train[i]) * x_train[i]\n",
        "    \n",
        "    # Updating the weights and intercepts by the gradients * learning rate\n",
        "    w0 = w0 - learning_rate * gradient_w0\n",
        "    w1 = w1 - learning_rate * gradient_w1\n",
        "\n",
        "\n",
        "# Compute total loss of test data\n",
        "total_loss = 0\n",
        "for i in range(len(x_test)):\n",
        "    total_loss += ((-y_test[i]) * np.log(sigmoid(w0 + w1 * x_test[i])) - (1 - y_test[i]) * np.log(1-sigmoid(w0 + w1 * x_test[i])))   \n",
        "\n",
        "\n",
        "plt.plot(range(750), cost, '-') \n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "print('Cross Entropy Error of my prediction and ground truth:', total_loss)\n",
        "print('Weight:', w1)\n",
        "print('Intercept:', w0)\n",
        "\n"
      ],
      "metadata": {
        "id": "mE6N6TJYT91J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}