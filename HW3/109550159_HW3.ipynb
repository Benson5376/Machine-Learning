{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your implementations right here to check your result\n",
    "# (Of course you can add your classes not written here)\n",
    "def entropy(sequnence):\n",
    "    \"\"\"\n",
    "    x : The label array\n",
    "    \"\"\"\n",
    "    _, clsCount = np.unique(sequnence, return_counts=True)\n",
    "    total = len(sequnence)\n",
    "    p = clsCount / total\n",
    "    log_p = np.log2(p)\n",
    "    ent = -np.sum(p*log_p)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def gini(sequnence):\n",
    "    \"\"\"\n",
    "    x : The label array\n",
    "    \"\"\"\n",
    "    _, clsCount = np.unique(sequnence, return_counts=True)\n",
    "    total = len(sequnence)\n",
    "    p = clsCount/total\n",
    "    gini = np.sum(np.square(p))\n",
    "    gini = 1 - gini\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_df.columns.values[:-1]\n",
    "x_train = train_df.iloc[:, :-1].to_numpy()\n",
    "y_train = train_df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = val_df.iloc[:, :-1].to_numpy()\n",
    "y_val = val_df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterionDict = {'gini': gini, 'entropy': entropy}\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, feature, threshold):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.n_data = None\n",
    "        self.metric = None\n",
    "        self.lchild = None\n",
    "        self.rchild = None\n",
    "        self.label = None\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=3):\n",
    "        if max_depth is None:\n",
    "            self.max_depth = np.inf\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "        if self.max_depth <= 0:\n",
    "            raise ValueError('Unexpected max depth')\n",
    "        if self.max_depth != np.inf and type(max_depth) != int:\n",
    "            raise TypeError('Max depth should be an integer.')\n",
    "        self.criterion = criterionDict[criterion]\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "        self.n_feature = None\n",
    "        self.leaflabel = []\n",
    "        self.nodeFeat = []\n",
    "\n",
    "    # Build the decision tree\n",
    "    def fit(self, x, y):\n",
    "        self.n_feature = len(x[0])\n",
    "        featureIdx = np.arange(self.n_feature)\n",
    "        self.root = self.CART_makeTree(feature=featureIdx,\n",
    "                                       dataset=x, label=y, currDepth=0)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = []\n",
    "        for i in range(len(x)):\n",
    "            xi = x[i]\n",
    "            currNode = self.root\n",
    "            while currNode.label is None:  # Split node\n",
    "                ftIdx = currNode.feature\n",
    "                if(xi[ftIdx] < currNode.threshold):\n",
    "                    currNode = currNode.lchild\n",
    "                else:\n",
    "                    currNode = currNode.rchild\n",
    "            prediction = currNode.label\n",
    "            y_pred.append(prediction)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        acc = (y_pred == y).mean()\n",
    "        return acc\n",
    "\n",
    "    def get_Feature(self, x, dataIdx):\n",
    "        currNode = self.root\n",
    "        while currNode.label is None:  # Split node\n",
    "            ftIdx = currNode.feature\n",
    "            if(x[ftIdx] < currNode.threshold):\n",
    "                print(dataIdx, currNode.feature)\n",
    "                currNode = currNode.lchild\n",
    "            else:\n",
    "                print(dataIdx, currNode.feature)\n",
    "                currNode = currNode.rchild\n",
    "\n",
    "    def decision_path(self, x):\n",
    "        i = 0\n",
    "        for data in x:\n",
    "            self.get_Feature(data, i)\n",
    "            i += 1\n",
    "\n",
    "    def CART_makeTree(self, feature, dataset, label, currDepth):\n",
    "        # Tree depth reach the max depth\n",
    "        u, counts = np.unique(label, return_counts=True)\n",
    "\n",
    "        if currDepth > self.max_depth or (len(feature)-1) <= 0:\n",
    "            node = Node(feature=None, threshold=None)\n",
    "            node.label = u[np.argmax(counts)]\n",
    "            self.leaflabel.append(node.label)\n",
    "            return node\n",
    "\n",
    "        # The dataset of node has same classification\n",
    "        elif counts[0] == len(label):\n",
    "            node = Node(feature=None, threshold=None)\n",
    "            node.label = u[0]\n",
    "            self.leaflabel.append(node.label)\n",
    "            return node\n",
    "\n",
    "        else:\n",
    "            bestFeatureIdx, nodeThresh, minGain = self.BestFeature(\n",
    "                                                       feature, dataset, label)\n",
    "            bestFeature = feature[bestFeatureIdx]\n",
    "            self.nodeFeat.append(bestFeature)\n",
    "            # The index of data which less than threshold in best feature\n",
    "            l_branchIdx = np.argwhere(dataset[:, bestFeature] < nodeThresh)\n",
    "            r_branchIdx = np.argwhere(dataset[:, bestFeature] >= nodeThresh)\n",
    "            l_train_x = dataset[l_branchIdx].reshape(-1, self.n_feature)\n",
    "            r_train_x = dataset[r_branchIdx].reshape(-1, self.n_feature)\n",
    "            l_train_y = label[l_branchIdx]\n",
    "            r_train_y = label[r_branchIdx]\n",
    "\n",
    "            # Recursively build left and subtree\n",
    "            if len(l_branchIdx) != 0 and len(r_branchIdx) != 0:\n",
    "                # Still need to split the branch\n",
    "                root = Node(bestFeature, nodeThresh)\n",
    "                root.n_data = len(feature)\n",
    "                root.metric = minGain\n",
    "                currDepth += 1\n",
    "                root.lchild = self.CART_makeTree(\n",
    "                              feature, l_train_x, l_train_y, currDepth)\n",
    "                root.rchild = self.CART_makeTree(\n",
    "                              feature, r_train_x, r_train_y, currDepth)\n",
    "                return root\n",
    "            else:\n",
    "                # All of data in this node are lower or higher than threshold,\n",
    "                # return a leaf.\n",
    "                node = Node(feature=None, threshold=None)\n",
    "                node.label = u[np.argmax(counts)]\n",
    "                self.leaflabel.append(node.label)\n",
    "                return node\n",
    "\n",
    "    def BestFeature(self, feature, dataset, label):\n",
    "        n_data = len(dataset)\n",
    "        minGain = np.inf\n",
    "        bestFeatureIdx = -1\n",
    "        nodeThresh = 0\n",
    "\n",
    "        for i in feature:\n",
    "            featureVals = dataset[:, i]\n",
    "            # N-1 threshold\n",
    "            for j in range(n_data-1):\n",
    "                threshold = (featureVals[j] + featureVals[j+1])/2\n",
    "                l_branchIdx = np.argwhere(featureVals < threshold)\n",
    "                r_branchIdx = np.argwhere(featureVals >= threshold)\n",
    "                l_branchLabel = label[l_branchIdx]\n",
    "                r_branchLabel = label[r_branchIdx]\n",
    "                l_branch_w = len(l_branchIdx) / n_data\n",
    "                r_branch_w = len(r_branchIdx) / n_data\n",
    "                # The impurity after spilt\n",
    "                gain = l_branch_w * self.criterion(l_branchLabel)\n",
    "                gain += r_branch_w * self.criterion(r_branchLabel)\n",
    "                if gain < minGain:\n",
    "                    minGain = gain\n",
    "                    bestFeatureIdx = np.argwhere(feature == i).item(0)\n",
    "                    nodeThresh = threshold\n",
    "\n",
    "        return bestFeatureIdx, nodeThresh, minGain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433333333333334"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_depth3.fit(x=x_train, y=y_train)\n",
    "clf_depth3.score(x=x_val, y=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_depth10.fit(x=x_train, y=y_train)\n",
    "clf_depth10.score(x=x_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433333333333334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.fit(x=x_train, y=y_train)\n",
    "clf_gini.score(x=x_val, y=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(x=x_train, y=y_train)\n",
    "clf_entropy.score(x=x_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "featIdx, n_show = np.unique(clf_depth10.nodeFeat, return_counts=True)\n",
    "selectedFeature = feature[featIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "y_pos = np.arange(len(selectedFeature))\n",
    "ax.barh(y_pos, n_show, align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(selectedFeature)\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators,\n",
    "                criterion='gini', max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.alpha_list = []\n",
    "        self.weakLearner = []\n",
    "        \n",
    "    def weakResult(self, clf, x, y):\n",
    "        y_pred = clf.predict(x)\n",
    "        err_list = (y_pred != y).astype(int).reshape(-1,1)\n",
    "        return err_list, y_pred.reshape(-1,1)\n",
    "    \n",
    "    def getWeakLearner(self, x, y, weights):\n",
    "        w_min = weights.min()\n",
    "        new_X = (weights*x)/w_min\n",
    "        weakclf = DecisionTree(criterion=self.criterion,\n",
    "                               max_depth=self.max_depth)\n",
    "        weakclf.fit(new_X, y)\n",
    "        err_list, pred_list = self.weakResult(weakclf, x, y)\n",
    "        err_w = (weights.T@err_list)[0,0] # weighted error\n",
    "        return weakclf, pred_list, err_w\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        y[y == 0] = -1\n",
    "        n_data = x.shape[0]\n",
    "        weights = (np.ones(n_data) / n_data).reshape(-1, 1)\n",
    "        for i in range(self.n_estimators):\n",
    "            # get new weak learner\n",
    "            weakclf, pred_list, error = self.getWeakLearner(x, y, weights)\n",
    "            alpha = 0.5*np.log((1-error)/max(error, 1e-8))\n",
    "            # Update weights\n",
    "            self.weakLearner.append(weakclf)\n",
    "            self.alpha_list.append(alpha)\n",
    "            weights = weights*np.exp(-1*alpha*y.reshape(-1,1)*pred_list) \n",
    "            weights = weights/weights.sum()\n",
    "        y[y == -1] = 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = np.zeros(len(x))\n",
    "        for i in range(self.n_estimators):\n",
    "            y_pred += self.alpha_list[i] * self.weakLearner[i].predict(x)\n",
    "        return np.sign(y_pred)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred[y_pred == -1] = 0\n",
    "        acc = (y_pred == y).mean()\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8933333333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AB_10tree = AdaBoost(n_estimators=10)\n",
    "AB_10tree.fit(x_train, y_train)\n",
    "AB_10tree.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8933333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AB_100tree = AdaBoost(n_estimators=100)\n",
    "AB_100tree.fit(x_train, y_train)\n",
    "AB_100tree.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features,\n",
    "                 criterion='gini', max_depth=None, bootstrap=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_feature = int(max_features)\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.bootstrap = bootstrap\n",
    "        self.forest = []\n",
    "        self.selectedFeature = []\n",
    "        self.n_dataPerTree = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        if not self.bootstrap:\n",
    "            self.n_dataPerTree = int(len(x)/self.n_estimators)\n",
    "        n_features = x.shape[1]\n",
    "        featureIdx = np.arange(n_features)\n",
    "        for i in range(self.n_estimators):\n",
    "            # Randomly pick feature\n",
    "            ft_subset = featureIdx.copy()\n",
    "            np.random.shuffle(ft_subset)\n",
    "            ft_subset = ft_subset[:self.max_feature]\n",
    "            self.selectedFeature.append(ft_subset)\n",
    "            # abstract corresponding data\n",
    "            new_x = x[:, ft_subset]\n",
    "            if not self.bootstrap:\n",
    "                n_data = len(x)\n",
    "                new_dataIdx = np.arange(n_data)\n",
    "                np.random.shuffle(new_dataIdx)\n",
    "                new_dataIdx = new_dataIdx[:self.n_dataPerTree]\n",
    "                new_x = new_x[new_dataIdx]\n",
    "                x = np.delete(x, new_dataIdx)\n",
    "            # Add a tree into forest\n",
    "            clf = DecisionTree(criterion=self.criterion,\n",
    "                               max_depth=self.max_depth)\n",
    "            clf.fit(new_x, y)\n",
    "            self.forest.append(clf)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred_all = []\n",
    "        n_test = len(x)\n",
    "        for i in range(self.n_estimators):\n",
    "            data_sf = x[:, self.selectedFeature[i]]\n",
    "            # predicted label of a certain tree\n",
    "            y_pred_e = self.forest[i].predict(data_sf)\n",
    "            y_pred_all.append(y_pred_e)\n",
    "        # Decide label by majority count\n",
    "        y_pred_all = np.array(y_pred_all)\n",
    "        y_pred = []\n",
    "        for i in range(n_test):\n",
    "            u, c = np.unique(y_pred_all[:, i], return_counts=True)\n",
    "            pred_label_f = u[np.argmax(c)]\n",
    "            y_pred.append(pred_label_f)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        acc = (y_pred == y).mean()\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=int(np.sqrt(x_train.shape[1])))\n",
    "clf_10tree.fit(x_train, y_train)\n",
    "clf_10tree.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=int(np.sqrt(x_train.shape[1])))\n",
    "clf_100tree.fit(x_train, y_train)\n",
    "clf_100tree.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=int(np.sqrt(x_train.shape[1])))\n",
    "clf_random_features.fit(x_train, y_train)\n",
    "clf_random_features.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566666666666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "clf_all_features.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training\n",
    "    x_train = data.iloc[:, :-1].to_numpy()\n",
    "    y_train = data.iloc[:, -1].to_numpy()\n",
    "    model = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
    "    model.fit(x_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([pd.read_csv('train.csv'), pd.read_csv('val.csv')])\n",
    "my_model = train_your_model(train_df)\n",
    "my_model.score(x_val, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File y_test.csv does not exist: 'y_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3e8fa7b3d76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price_range'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test-set accuarcy score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/paul2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/paul2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/paul2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/paul2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/paul2/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File y_test.csv does not exist: 'y_test.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree(criterion='gini', max_depth=3) failed\n",
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "AdaBoost(n_estimators=10) failed\n",
      "Considering the randomness, we will check it manually\n",
      "AdaBoost(n_estimators=100) failed\n",
      "Considering the randomness, we will check it manually\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 25.0 ~ 50.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c9afcc29181526987d0b385fb14ac7bec129a3a1f159e207d0db8e842e65205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
